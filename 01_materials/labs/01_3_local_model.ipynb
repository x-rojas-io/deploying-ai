{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "391397a5",
   "metadata": {},
   "source": [
    "# LM Studio\n",
    "\n",
    " LM Studio is a powerful platform designed for working with large language models (LLMs) on your local machine. It provides an intuitive interface for downloading, managing, and running a wide variety of open-source LLMs.\n",
    "\n",
    "A few characteristics of LM Studio are:\n",
    "\n",
    "+ With LM Studio, users can experiment with different models, customize their settings, and interact with them in real time.\n",
    "+ The platform supports both CPU and GPU acceleration, making it accessible for a range of hardware configurations.\n",
    "+ LM Studio is ideal for developers, researchers, and enthusiasts who want to explore LLM capabilities without relying on cloud-based solutions.\n",
    "\n",
    "You can download and install LMStudio from [lmstudio.ai](https://lmstudio.ai/).\n",
    "\n",
    "## LM Studio API\n",
    "\n",
    "An important characteristic in LM Studio is its support for OpenAI's API:\n",
    "\n",
    "+ LM Studio accepts requests on several OpenAI endpoints and returns OpenAI-like response objects ([lmstudio.ai](https://platform.openai.com/docs/guides/prompt-engineering#message-roles-and-instruction-following)).\n",
    "+ This means that we can reuse our code by pointing the local API using the `base_url` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc96b0b",
   "metadata": {},
   "source": [
    "## Setup LM Studio\n",
    "\n",
    "+ Download and install LM Studio from [lmstudio.ai](https://lmstudio.ai/).\n",
    "+ Start the application.\n",
    "+ Select a model to load:\n",
    "\n",
    "    - From the developer tab, you can select a model from the top bar.\n",
    "    - Select a model, for example, any model from the Qwen3 class of models.\n",
    "    - Start the server with the control on the top-left labelled \"Status: Stopped\" or \"Status: Running\".\n",
    "\n",
    "\n",
    "<img src=\"./img/01_lmstudio.png\" height=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f3a04b",
   "metadata": {},
   "source": [
    "With the setup above, we can point to the local server as indicated below. The API works similarly to OpenAI; however, the underlying model runs locally and was not necessarily produced by OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93f5989",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(base_url=\"http://127.0.0.1:1234/v1\", api_key=\"not-needed\")\n",
    "\n",
    "response = client.responses.create(\n",
    "    model = 'local-model',\n",
    "    input = 'Hello world!',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1a1fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(response.output_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2551b016",
   "metadata": {},
   "source": [
    "## Using an Image as Context\n",
    "\n",
    "There are several ways to interact with the client and use an image as context. They are discussed in the OpenAI platform [documentation](https://platform.openai.com/docs/guides/images-vision?format=base64-encoded#analyze-images). LMStudio inherits this behaviour as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173e1c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "# Path to your image\n",
    "image_path = \"./img/00_cat.jpg\"\n",
    "\n",
    "# Getting the Base64 string\n",
    "base64_image = encode_image(image_path)\n",
    "\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                { \"type\": \"input_text\", \"text\": \"what's in this image?\" },\n",
    "                {\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"image_url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb68647d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(response.output_text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deploying-ai-env (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
