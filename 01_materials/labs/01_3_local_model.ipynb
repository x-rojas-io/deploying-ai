{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "391397a5",
   "metadata": {},
   "source": [
    "# LM Studio\n",
    "\n",
    " LM Studio is a powerful platform designed for working with large language models (LLMs) on your local machine. It provides an intuitive interface for downloading, managing, and running a wide variety of open-source LLMs.\n",
    "\n",
    "A few characteristics of LM Studio are:\n",
    "\n",
    "+ With LM Studio, users can experiment with different models, customize their settings, and interact with them in real time.\n",
    "+ The platform supports both CPU and GPU acceleration, making it accessible for a range of hardware configurations.\n",
    "+ LM Studio is ideal for developers, researchers, and enthusiasts who want to explore LLM capabilities without relying on cloud-based solutions.\n",
    "\n",
    "You can download and install LMStudio from [lmstudio.ai](https://lmstudio.ai/).\n",
    "\n",
    "## LM Studio API\n",
    "\n",
    "An important characteristic in LM Studio is its support for OpenAI's API:\n",
    "\n",
    "+ LM Studio accepts requests on several OpenAI endpoints and returns OpenAI-like response objects ([lmstudio.ai](https://platform.openai.com/docs/guides/prompt-engineering#message-roles-and-instruction-following)).\n",
    "+ This means that we can reuse our code by pointing the local API using the `base_url` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc96b0b",
   "metadata": {},
   "source": [
    "## Setup LM Studio\n",
    "\n",
    "+ Download and install LM Studio from [lmstudio.ai](https://lmstudio.ai/).\n",
    "+ Start the application.\n",
    "+ Select a model to load:\n",
    "\n",
    "    - From the developer tab, you can select a model from the top bar.\n",
    "    - Select a model, for example, any model from the Qwen3 class of models.\n",
    "    - Start the server with the control on the top-left labelled \"Status: Stopped\" or \"Status: Running\".\n",
    "\n",
    "\n",
    "<img src=\"./img/01_lmstudio.png\" height=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f3a04b",
   "metadata": {},
   "source": [
    "With the setup above, we can point to the local server as indicated below. The API works similarly to OpenAI; however, the underlying model runs locally and was not necessarily produced by OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b93f5989",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI(base_url=\"http://127.0.0.1:1234/v1\", api_key=\"not-needed\")\n",
    "\n",
    "response = client.responses.create(\n",
    "    model = 'local-model',\n",
    "    input = 'Hello world!',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d1a1fa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hello, world! üåç  \n",
       "It‚Äôs great to meet you! How can I assist you today? Whether you need help with coding, learning, creativity, or just want to chat ‚Äî I‚Äôm here for you. üòä"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "\n",
    "display(Markdown(response.output_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2551b016",
   "metadata": {},
   "source": [
    "## Using an Image as Context\n",
    "\n",
    "There are several ways to interact with the client and use an image as context. They are discussed in the OpenAI platform [documentation](https://platform.openai.com/docs/guides/images-vision?format=base64-encoded#analyze-images). LMStudio inherits this behaviour as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "173e1c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "# Path to your image\n",
    "image_path = \"./img/00_cat.jpg\"\n",
    "\n",
    "# Getting the Base64 string\n",
    "base64_image = encode_image(image_path)\n",
    "\n",
    "\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4.1\",\n",
    "    input=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                { \"type\": \"input_text\", \"text\": \"what's in this image?\" },\n",
    "                {\n",
    "                    \"type\": \"input_image\",\n",
    "                    \"image_url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                },\n",
    "            ],\n",
    "        }\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb68647d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This image features a **striped tabby cat** sitting on a weathered, light-colored stone or concrete ledge.\n",
       "\n",
       "Here are some key details:\n",
       "\n",
       "- **The Cat**: It‚Äôs a short-haired tabby with a classic brown and black striped pattern. The cat has a very alert posture, sitting upright and looking directly at the camera with bright green eyes.\n",
       "- **The Setting**: The cat is outdoors, with a background of bare tree branches against a pale, slightly overcast sky. The lighting is soft and even.\n",
       "- **The Composition**: The cat is the clear focal point, positioned centrally. The shallow depth of field blurs the background, drawing attention to the cat‚Äôs face and expression.\n",
       "- **The Mood**: The image conveys a sense of calm vigilance. The cat appears observant and possibly a bit wary, as if it‚Äôs surveying its surroundings.\n",
       "\n",
       "In summary, this is a portrait of a tabby cat sitting outdoors, looking directly at the viewer, set against a natural, slightly blurred backdrop of bare trees and sky."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response.output_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üí° Frequently Asked Questions (FAQ)\n",
    "\n",
    "**Q: What is Ollama?**\n",
    "A: Ollama is a tool that allows you to run open-source LLMs (like Llama 3, Mistral, Gemma) locally on your own machine. It handles the model weights and provides an API that mimics OpenAI's.\n",
    "\n",
    "**Q: Why run models locally?**\n",
    "A: \n",
    "1. **Privacy**: Data never leaves your computer.\n",
    "2. **Cost**: No per-token API fees.\n",
    "3. **Offline**: Works without internet.\n",
    "\n",
    "**Q: Why is it slower than OpenAI?**\n",
    "A: Local models run on your consumer hardware (CPU/GPU). Cloud providers use clusters of massive enterprise GPUs (H100s). Performance depends heavily on your RAM and Graphics Card.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deploying-ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
