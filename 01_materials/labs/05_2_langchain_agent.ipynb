{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c23d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv ../../05_src/.secrets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b069082",
   "metadata": {},
   "source": [
    "# Introduction to LangGraph\n",
    "\n",
    "LangGraph is a low-level agent orchestration framework. It integrates with LangChain, but can also operate without it. \n",
    "\n",
    "We will build a graph to demonstrate how LangGraph integrates different models and tools.\n",
    "\n",
    "There are six steps in this demo which is roughly based on [LangGraph's Quickstart](https://docs.langchain.com/oss/python/langgraph/quickstart):\n",
    "\n",
    "1. Define tools and models\n",
    "2. Define state\n",
    "3. Define model node\n",
    "4. Deine tool node\n",
    "5. Define end logic\n",
    "6. Build and compile the agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94d489d",
   "metadata": {},
   "source": [
    "# Define tools and model\n",
    "\n",
    "We will use an OpenAI model and a tools that query public APIs to illustrate tools usage. \n",
    "\n",
    "The APIs are:\n",
    "- [Meowfacts API](https://github.com/wh-iterabb-it/meowfacts)\n",
    "- [Dog Facts API](https://kinduff.github.io/dog-api/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d360b9",
   "metadata": {},
   "source": [
    "Start by initializing a [chat model](https://docs.langchain.com/oss/python/langchain/models#initialize-a-model) in LangChain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6aa598",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "import os\n",
    "\n",
    "model = init_chat_model(\n",
    "    \"openai:gpt-4o-mini\",\n",
    "    temperature=0.7,\n",
    "    base_url='https://k7uffyg03f.execute-api.us-east-1.amazonaws.com/prod/openai/v1', \n",
    "    api_key='any value',\n",
    "    default_headers={\"x-api-key\": os.getenv('API_GATEWAY_KEY')}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ac0ea7",
   "metadata": {},
   "source": [
    "The model component can directly be used, however we will make it part of a larger object called a Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580ddeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.invoke(\"Why do parrots talk?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e73623",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89daac1",
   "metadata": {},
   "source": [
    "Using LangChain, define tools. Notice the `@tool` decorator on each key function definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1574a191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "import requests\n",
    "import json\n",
    "\n",
    "@tool\n",
    "def get_cat_facts(n:int=1):\n",
    "    \"\"\"\n",
    "    Returns n cat facts from the Meowfacts API.\n",
    "    \"\"\"\n",
    "    url = \"https://meowfacts.herokuapp.com/\"\n",
    "    params = {\n",
    "        \"count\": n\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    resp_dict = json.loads(response.text)\n",
    "    facts_list = resp_dict.get(\"data\", [])\n",
    "    facts = \"\\n\".join([f\"{i+1}. {fact}\\n\" for i, fact in enumerate(facts_list)])\n",
    "    return facts\n",
    "\n",
    "@tool\n",
    "def get_dog_facts(n:int=1):\n",
    "    \"\"\"\n",
    "    Returns n dog facts from the Dog API.\n",
    "    \"\"\"\n",
    "    url = \"http://dogapi.dog/api/v2/facts\"\n",
    "    params = {\n",
    "        \"limit\": n\n",
    "    }\n",
    "    response = requests.get(url, params=params)\n",
    "    resp_dict = json.loads(response.text)\n",
    "    facts_list = resp_dict.get(\"data\", [])\n",
    "    facts = \"\\n\".join([f\"{i+1}. {fact['attributes']['body']}\\n\" for i, fact in enumerate(facts_list)])\n",
    "    return facts\n",
    "\n",
    "\n",
    "# Augment the LLM with tools\n",
    "tools = [get_cat_facts, get_dog_facts]\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "model_with_tools = model.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88cde3b",
   "metadata": {},
   "source": [
    "# Define state\n",
    "\n",
    "The graph's state is used to store:\n",
    "\n",
    "+ The message history.\n",
    "+ The number of LLM calls.\n",
    "\n",
    "This is an extension of `langgraph.MessagesState`, which does not store the number of LLM calls. The [`Annotated`](https://docs.python.org/3/library/typing.html#typing.Annotated) type with [`operator.add`](https://docs.python.org/3/library/operator.html#module-operator) ensures that new messages are appended to the existing list rather than replacing it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6d9b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import AnyMessage\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "import operator\n",
    "\n",
    "\n",
    "class MessagesState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]\n",
    "    llm_calls: int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53adbee6",
   "metadata": {},
   "source": [
    "# Define Model Node\n",
    "\n",
    "The model node is used to call the LLM and decide wheter to call the tool or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b02e770",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "\n",
    "def llm_call(state: dict):\n",
    "    \"\"\"LLM decides whether to call a tool or not\"\"\"\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            model_with_tools.invoke(\n",
    "                [\n",
    "                    SystemMessage(\n",
    "                        content=\"You are a helpful assistant tasked with stating interesting and fun facts about cats and dogs.\"\n",
    "                    )\n",
    "                ]\n",
    "                + state[\"messages\"]\n",
    "            )\n",
    "        ],\n",
    "        \"llm_calls\": state.get('llm_calls', 0) + 1\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66b3525",
   "metadata": {},
   "source": [
    "# Define Tool Node\n",
    "\n",
    "The tool node is used to call the tools and return the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac415a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "\n",
    "def tool_node(state: dict):\n",
    "    \"\"\"Performs the tool call\"\"\"\n",
    "\n",
    "    result = []\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        tool = tools_by_name[tool_call[\"name\"]]\n",
    "        observation = tool.invoke(tool_call[\"args\"])\n",
    "        result.append(ToolMessage(content=observation, tool_call_id=tool_call[\"id\"]))\n",
    "    return {\"messages\": result}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f5929fa",
   "metadata": {},
   "source": [
    "# Define End Logic\n",
    "\n",
    "The conditional edge function is used to route to the tool node or end based upon whether the LLM made a tool call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453bf397",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "\n",
    "def should_continue(state: MessagesState) -> Literal[\"tool_node\", END]:\n",
    "    \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\"\n",
    "\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "\n",
    "    # If the LLM makes a tool call, then perform an action\n",
    "    if last_message.tool_calls:\n",
    "        return \"tool_node\"\n",
    "\n",
    "    # Otherwise, we stop (reply to the user)\n",
    "    return END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d46f73",
   "metadata": {},
   "source": [
    "# Build and Compile the Agent\n",
    "\n",
    "The agent is built using the [`StateGraph`](https://reference.langchain.com/python/langgraph/graphs/?_gl=1*z2scpt*_gcl_au*MTczODQwMTgwNC4xNzU5MjU2OTcw*_ga*NDE4MTM5NDc2LjE3NTkyNTY5NzA.*_ga_47WX3HKKY2*czE3NjE0MjEyMDQkbzMyJGcxJHQxNzYxNDI0MTcxJGoxOCRsMCRoMA..#langgraph.graph.state.StateGraph) class and compiled using the [`compile`](https://reference.langchain.com/python/langgraph/graphs/?_gl=1*1dnnqxy*_gcl_au*MTczODQwMTgwNC4xNzU5MjU2OTcw*_ga*NDE4MTM5NDc2LjE3NTkyNTY5NzA.*_ga_47WX3HKKY2*czE3NjE0MjkwMjUkbzMzJGcwJHQxNzYxNDI5MDI1JGo2MCRsMCRoMA..#langgraph.graph.state.StateGraph.compile) method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03be419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build workflow\n",
    "agent_builder = StateGraph(MessagesState)\n",
    "\n",
    "# Add nodes\n",
    "agent_builder.add_node(\"llm_call\", llm_call)\n",
    "agent_builder.add_node(\"tool_node\", tool_node)\n",
    "\n",
    "# Add edges to connect nodes\n",
    "agent_builder.add_edge(START, \"llm_call\")\n",
    "agent_builder.add_conditional_edges(\n",
    "    \"llm_call\",\n",
    "    should_continue,\n",
    "    [\"tool_node\", END]\n",
    ")\n",
    "agent_builder.add_edge(\"tool_node\", \"llm_call\")\n",
    "\n",
    "# Compile the agent\n",
    "agent = agent_builder.compile()\n",
    "\n",
    "# Show the agent\n",
    "from IPython.display import Image, display\n",
    "display(Image(agent.get_graph(xray=True).draw_mermaid_png()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846bdc9b",
   "metadata": {},
   "source": [
    "We can run the model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492f8cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invoke\n",
    "from langchain_core.messages import HumanMessage\n",
    "messages = [HumanMessage(content=\"Tell me 3 things about cats.\"), HumanMessage(content=\"Now tell me 2 things about dogs.\")]\n",
    "messages = agent.invoke({\"messages\": messages})\n",
    "for m in messages[\"messages\"]:\n",
    "    m.pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deploying-ai-env (3.11.13)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
